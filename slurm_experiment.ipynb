{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2cb80e4b",
   "metadata": {},
   "source": [
    "# ðŸ§ª Reproducible Slurm Cluster Deployment with Infrastructure Manager\n",
    "\n",
    "This notebook demonstrates how to use the [Infrastructure Manager (IM)](https://imdocs.readthedocs.io/en/devel/) to deploy a Slurm cluster on [Chameleon Cloud](https://www.chameleoncloud.org/) via the OpenStack-based site **KVM@TACC**.\n",
    "\n",
    "**Goals:**\n",
    "- Install the IM client\n",
    "- Deploy a Slurm cluster using a TOSCA template\n",
    "- Submit a test job to Slurm and retrieve the output\n",
    "- Destroy the deployed infrastructure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d73d1bd8",
   "metadata": {},
   "source": [
    "## ðŸ› ï¸ Step 1: Install the Infrastructure Manager CLI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d05e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install im-client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad63bf7",
   "metadata": {},
   "source": [
    "## ðŸ” Step 2: Authentication File Setup\n",
    "\n",
    "The IM client uses an authentication file. The file, `imcham-auth.dat`, has two lines:\n",
    "\n",
    "- **Infrastructure Manager credentials** - Obtain an access token from [EGI Check-in Token Portal](https://aai.egi.eu/token) and save it in the `token` variable in the authentication file. Access tokens expire every hour.\n",
    "\n",
    "![title](images/egi-portal.png)\n",
    "\n",
    "- **Chameleon Cloud credentials** â€” To get the Chameleon credentials, head to the [Chameleon Cloud page](https://chameleoncloud.org/) and follow these steps:\n",
    "> 1. Create a user account in Chameleon.\n",
    "> 2. Since we are interested in OpenStack, select KVM@TACC (under Experiments), an OpenStack site provided by the Texas Advanced Computing Center (TACC) based on the KVM (Kernel-based Virtual Machine) hypervisor.\n",
    ">\n",
    "> ![title](images/chameleon-kvmtacc.png)\n",
    ">\n",
    "> 3. In the Identity section, one can create Application Credentials. Select **Create application credential**.\n",
    ">\n",
    "> ![title](images/chameleon-app-cred.png)\n",
    ">> Application Credentials allow user applications to authenticate to keystone. With application credentials, applications authenticate with the application credential ID and a secret string which is not the userâ€™s password. This way, the userâ€™s password is not embedded in the applicationâ€™s configuration.\n",
    ">\n",
    "> 4. In the name field, specify a name and leave the rest of the options as default. \n",
    ">\n",
    "> ![title](images/chameleon-create-cred.png)\n",
    ">\n",
    "> Once obtained the application credentials, you can download the openrc file or the clouds.yaml file. The Project ID is also shown in the list of credentials after closing the pop-up.\n",
    "\n",
    "Now, substitute the second line of the authentication file with the credentials obtained in Chameleon.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d425a8",
   "metadata": {},
   "source": [
    "## ðŸ“„ Step 3: Define the Infrastructure Template\n",
    "\n",
    "Below is a sample definition for a Slurm cluster infrastructure using a TOSCA template.\n",
    "\n",
    "We save it to a file called `slurm-cluster.yaml`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "33fd9efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "slurm_cluster_recipe = \"\"\"\n",
    "tosca_definitions_version: tosca_simple_yaml_1_0\n",
    "\n",
    "description: Deploy a SLURM Virtual Cluster.\n",
    "imports:\n",
    "- grycap_custom_types: https://raw.githubusercontent.com/grycap/tosca/main/custom_types.yaml\n",
    "metadata:\n",
    "  childs: []\n",
    "  display_name: SLURM virtual cluster\n",
    "  filename: slurm_cluster.yml\n",
    "  icon: images/slurm.png\n",
    "  infra_name: slurm_test\n",
    "  order: 4\n",
    "  tabs:\n",
    "    FE Node Features: fe_.*\n",
    "    SLURM Features: slurm_.*\n",
    "    WNs Features: wn_.*\n",
    "  template_name: SLURM\n",
    "  template_version: 1.1.2\n",
    "topology_template:\n",
    "  inputs:\n",
    "    fe_cpus:\n",
    "      default: 1\n",
    "      description: Number of CPUs for the front-end node\n",
    "      required: true\n",
    "      type: integer\n",
    "    fe_disk_size:\n",
    "      constraints:\n",
    "      - valid_values:\n",
    "        - 0 GiB\n",
    "        - 10 GiB\n",
    "        - 20 GiB\n",
    "        - 50 GiB\n",
    "        - 100 GiB\n",
    "        - 200 GiB\n",
    "        - 500 GiB\n",
    "        - 1 TiB\n",
    "        - 2 TiB\n",
    "      default: 0 GiB\n",
    "      description: Size of the disk to be attached to the FE instance (Set 0 if disk\n",
    "        is not needed)\n",
    "      type: scalar-unit.size\n",
    "    fe_mem:\n",
    "      default: 1 GiB\n",
    "      description: Amount of Memory for the front-end node\n",
    "      required: true\n",
    "      type: scalar-unit.size\n",
    "    fe_mount_path:\n",
    "      default: /home/data\n",
    "      description: Path to mount the FE attached disk\n",
    "      type: string\n",
    "    fe_ports:\n",
    "      default:\n",
    "        port_22:\n",
    "          protocol: tcp\n",
    "          source: 22\n",
    "      description: 'List of ports to be Opened in FE node (eg. 22,80,443,2000:2100).\n",
    "\n",
    "        You can also include the remote CIDR (eg. 8.8.0.0/24).\n",
    "\n",
    "        '\n",
    "      entry_schema:\n",
    "        type: PortSpec\n",
    "      type: map\n",
    "    fe_volume_id:\n",
    "      default: ''\n",
    "      description: 'Or URL of the disk to be attached to the FE instance (format:\n",
    "        ost://api.cloud.ifca.es/'\n",
    "      type: string\n",
    "    slurm_version:\n",
    "      constraints:\n",
    "      - valid_values:\n",
    "        - 23.11.8\n",
    "        - 20.11.9\n",
    "        - 21.08.5\n",
    "        - 21.08.8\n",
    "        - 22.05.10\n",
    "      default: 23.11.8\n",
    "      description: Version of SLURM to be installed\n",
    "      type: string\n",
    "    wn_cpus:\n",
    "      default: 1\n",
    "      description: Number of CPUs for the WNs\n",
    "      required: true\n",
    "      type: integer\n",
    "    wn_disk_size:\n",
    "      constraints:\n",
    "      - valid_values:\n",
    "        - 0 GiB\n",
    "        - 10 GiB\n",
    "        - 20 GiB\n",
    "        - 50 GiB\n",
    "        - 100 GiB\n",
    "        - 200 GiB\n",
    "        - 500 GiB\n",
    "        - 1 TiB\n",
    "        - 2 TiB\n",
    "      default: 0 GiB\n",
    "      description: Size of the disk to be attached to the WN instances (Set 0 if disk\n",
    "        is not needed)\n",
    "      type: scalar-unit.size\n",
    "    wn_mem:\n",
    "      default: 1024\n",
    "      description: Amount of Memory for the WNs in MiB\n",
    "      required: true\n",
    "      type: integer\n",
    "    wn_mount_path:\n",
    "      default: /mnt/data\n",
    "      description: Path to mount the WN attached disk\n",
    "      type: string\n",
    "    wn_num:\n",
    "      default: 1\n",
    "      description: Number of WNs in the cluster\n",
    "      required: true\n",
    "      type: integer\n",
    "  node_templates:\n",
    "    fe_block_storage:\n",
    "      properties:\n",
    "        size:\n",
    "          get_input: fe_disk_size\n",
    "        volume_id:\n",
    "          get_input: fe_volume_id\n",
    "      type: tosca.nodes.BlockStorage\n",
    "    lrms_front_end:\n",
    "      properties:\n",
    "        version:\n",
    "          get_input: slurm_version\n",
    "        wn_cpus:\n",
    "          get_input: wn_cpus\n",
    "        wn_ips:\n",
    "          get_attribute:\n",
    "          - lrms_wn\n",
    "          - private_address\n",
    "        wn_mem:\n",
    "          get_input: wn_mem\n",
    "      requirements:\n",
    "      - host: lrms_server\n",
    "      type: tosca.nodes.indigo.LRMS.FrontEnd.Slurm\n",
    "    lrms_server:\n",
    "      capabilities:\n",
    "        endpoint:\n",
    "          properties:\n",
    "            dns_name: slurmserver\n",
    "            network_name: PUBLIC\n",
    "            ports:\n",
    "              get_input: fe_ports\n",
    "        host:\n",
    "          properties:\n",
    "            mem_size:\n",
    "              get_input: fe_mem\n",
    "            num_cpus:\n",
    "              get_input: fe_cpus\n",
    "        os:\n",
    "          properties:\n",
    "            distribution: ubuntu\n",
    "            image: ost://kvm.tacc.chameleoncloud.org/96d9c658-6540-4796-ae64-54d8ac6c45f8\n",
    "            type: linux\n",
    "      properties:\n",
    "        instance_name: slurm_test_lrms_server\n",
    "      requirements:\n",
    "      - local_storage:\n",
    "          node: fe_block_storage\n",
    "          relationship:\n",
    "            properties:\n",
    "              location:\n",
    "                get_input: fe_mount_path\n",
    "            type: AttachesTo\n",
    "      type: tosca.nodes.indigo.Compute\n",
    "    lrms_wn:\n",
    "      capabilities:\n",
    "        host:\n",
    "          properties:\n",
    "            mem_size:\n",
    "              concat:\n",
    "              - get_input: wn_mem\n",
    "              - ' MiB'\n",
    "            num_cpus:\n",
    "              get_input: wn_cpus\n",
    "        os:\n",
    "          properties:\n",
    "            distribution: ubuntu\n",
    "            image: ost://kvm.tacc.chameleoncloud.org/96d9c658-6540-4796-ae64-54d8ac6c45f8\n",
    "            type: linux\n",
    "        scalable:\n",
    "          properties:\n",
    "            count:\n",
    "              get_input: wn_num\n",
    "      properties:\n",
    "        instance_name: slurm_test_lrms_wn\n",
    "      requirements:\n",
    "      - local_storage:\n",
    "          node: wn_block_storage\n",
    "          relationship:\n",
    "            properties:\n",
    "              location:\n",
    "                get_input: wn_mount_path\n",
    "            type: AttachesTo\n",
    "      type: tosca.nodes.indigo.Compute\n",
    "    wn_block_storage:\n",
    "      properties:\n",
    "        size:\n",
    "          get_input: wn_disk_size\n",
    "      type: tosca.nodes.BlockStorage\n",
    "    wn_node:\n",
    "      properties:\n",
    "        front_end_ip:\n",
    "          get_attribute:\n",
    "          - lrms_server\n",
    "          - private_address\n",
    "          - 0\n",
    "        public_front_end_ip:\n",
    "          get_attribute:\n",
    "          - lrms_server\n",
    "          - public_address\n",
    "          - 0\n",
    "        version:\n",
    "          get_input: slurm_version\n",
    "      requirements:\n",
    "      - host: lrms_wn\n",
    "      type: tosca.nodes.indigo.LRMS.WorkerNode.Slurm\n",
    "  outputs:\n",
    "    cluster_creds:\n",
    "      value:\n",
    "        get_attribute:\n",
    "        - lrms_server\n",
    "        - endpoint\n",
    "        - credential\n",
    "        - 0\n",
    "    cluster_ip:\n",
    "      value:\n",
    "        get_attribute:\n",
    "        - lrms_server\n",
    "        - public_address\n",
    "        - 0\n",
    "\"\"\"\n",
    "\n",
    "# Save the template to a file\n",
    "with open(\"slurm-cluster.yaml\", \"w\") as f:\n",
    "    f.write(slurm_cluster_recipe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c48d19b8",
   "metadata": {},
   "source": [
    "## ðŸš€ Step 4: Deploy the Infrastructure\n",
    "\n",
    "Use the `im_client.py` CLI to deploy your infrastructure. The command will return an `infrastructure ID` used for future interactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc3ba89",
   "metadata": {},
   "outputs": [],
   "source": [
    "!im_client.py -r https://im.egi.eu/im/ -a ./imcham-auth.dat create slurm-cluster.yaml\n",
    "\n",
    "# The command is using the publicly available endpoint of the IM at https://im.egi.eu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f773e8eb",
   "metadata": {},
   "source": [
    "Once deployed, paste the infrastructure ID below to continue working with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "05b2d06b",
   "metadata": {},
   "outputs": [],
   "source": [
    "infra_id = \"infra-id\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e4fdbc",
   "metadata": {},
   "source": [
    "## ðŸ” Step 5: Check Deployment Status\n",
    "\n",
    "Before we SSH into the cluster, we need to check if the VM is in state configured:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13aee55e-184d-4198-94d1-390bf7971e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!im_client.py -r https://im.egi.eu/im -a ./imcham-auth.dat getstate {infra_id}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d2228fb",
   "metadata": {},
   "source": [
    "## ðŸ§ª Step 6: Submit a Slurm Job\n",
    "\n",
    "SSH into the frontend VM and run your Slurm job from there. To do that, run the following command and paste the output to create the SSH command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e228cb-103b-4d64-9a66-57f4d49538cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!im_client.py -r https://im.egi.eu/im -a ./imcham-auth.dat ssh {infra_id} 1 -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "id": "8b08fe89-cd9e-4303-80bc-b36eb959a7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ssh_command = \"ssh-command\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e938dbe-1699-455e-883b-2ae8b82c8710",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a SLURM job script\n",
    "script = \"\"\"#!/bin/bash\n",
    "#SBATCH --job-name=test\n",
    "#SBATCH --output=hostname.out\n",
    "\n",
    "hostname\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fbe4058-87e9-4ee5-a09d-2450956e5e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "# Connect to the cluster and save the job in /home/slurm/test.sh\n",
    "subprocess.run(f\"{ssh_command} 'echo {script!r} | sudo -u slurm tee /home/slurm/test.sh > /dev/null'\", shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02aee3c0-de8f-4f04-89f8-103f8d697a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Submit the script using 'sbatch' \n",
    "!$ssh_command \"sudo su - slurm -c 'sbatch /home/slurm/test.sh'\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "205b6bcc",
   "metadata": {},
   "source": [
    "## ðŸ“¤ Step 7: Check Output\n",
    "\n",
    "You can check the job status with `squeue` and inspect outputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cffe2bf-8b48-4ef4-baaf-3bba0f02c62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the status of the job with the 'squeue' command\n",
    "!$ssh_command \"sudo -u slurm squeue\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df854b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Once it finishes, check the output of the SLURM job\n",
    "!$ssh_command \"sudo -u slurm cat /home/slurm/hostname.out\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e422533",
   "metadata": {},
   "source": [
    "## ðŸ§¹ Step 8: Destroy the Infrastructure\n",
    "\n",
    "After completing the experiment, destroy the infrastructure to free up resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a0af251",
   "metadata": {},
   "outputs": [],
   "source": [
    "!im_client.py -r https://im.egi.eu/im -a ./imcham-auth.dat destroy {infra_id}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
